{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc46a60-e927-4d4b-bd79-6df07ee55622",
   "metadata": {},
   "source": [
    "**Lab details:**\n",
    "- Natural language Processing (NLP) - CS429\n",
    "- Lab Session-3\n",
    "- Date- 26/08/2025\n",
    "- Marks - 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3acabd-826a-4fdd-aeee-16998c78f71d",
   "metadata": {},
   "source": [
    "**Instruction:**\n",
    "1) The name of your Python file must follow this format: **studentID_Name**\n",
    "   - Example: If Student ID = 2022B3A70617P and Name = Nikhil Manvendra Singh\n",
    "   - File name should be: 2022B3A70617P_Nikhil_Manvendra_Singh.ipynb\n",
    "2) Submission Deadline: 6:15 PM (Today)\n",
    "3) Students are required to fill the blanks with code syntax in the section provided below and Attempt evaluative question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2d37c-6300-4599-9d34-066c9054fa8b",
   "metadata": {},
   "source": [
    "This Labsheet contains the basics of Naïve Bayes Classifier and an application in Sentiment Analysis. Classification lies at the heart of both human and machine intelligence. Sentiment Analysis here just refers to text categorization (classification) based on some sentiment, a positive or negative inclination towards any matter. The simplest version of sentiment analysis is a binary classification task, which we shall study in this labsheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e87d0f6-c369-463e-ac80-2146862e5e38",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------Lab Section--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0390b-01b4-4b32-b3a6-a24e601c4ca7",
   "metadata": {},
   "source": [
    "## Naïve Bayes Classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694900ac-cf86-4cab-9e68-201eb9865f8e",
   "metadata": {},
   "source": [
    "Naive Bayes algorithm is a probabilistic algorithm based on the famous \"Bayes Theorem\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd3af74-2235-438b-bbc7-ff2c21cde0d9",
   "metadata": {},
   "source": [
    "$$P(c|x) = \\frac{P(x|c) \\cdot P(c)}{P(x)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f95d0cd-f352-49b0-9791-07e0eb0fe314",
   "metadata": {},
   "source": [
    "where\n",
    "- 𝑃(𝑐|𝑥) is the posterior probability of class given predictor\n",
    "- 𝑃(𝑥|𝑐) is the probability of predictor given class \n",
    "- 𝑃(𝑐)is the probability of class \n",
    "- 𝑃(𝑥) is the prior probability of predictorx."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa9922f-61d9-48e1-aa00-5b64662aaa12",
   "metadata": {},
   "source": [
    "When we know how frequent a class occurs (𝑃(𝑐)), how frequent a predictor occurs (𝑃(𝑥)) and the probability of a predictor given the class (𝑃(𝑥|𝑐)), we can calculate how often a class occurs given the predictor (𝑃(𝑐|𝑥))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbf3a96-a238-491c-a4f2-5dc1f62181c2",
   "metadata": {},
   "source": [
    "In real-life machine learning problem, there are typically multiple predictors of a class instead of a single predictor 𝑥. Hence, the formula becomes more complicated considering the possible dependence between the predictors. Let's have a look at an example with 3 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4939fd-78dc-458d-8afd-1dbf767fe422",
   "metadata": {},
   "source": [
    "$$\n",
    "P(\\text{class} \\mid x_1, x_2, x_3) = \\frac{P(\\text{class} \\cap (x_1, x_2, x_3))}{P(x_1, x_2, x_3)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{class} \\mid x_1, x_2, x_3) \\propto P(\\text{class} \\cap x_1 \\cap x_2 \\cap x_3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{class} \\mid x_1, x_2, x_3) \\propto P(x_1 \\cap x_2 \\cap x_3 \\cap \\text{class})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02581e1-031e-4bc4-af3a-5aa33d5d1b1d",
   "metadata": {},
   "source": [
    "Next, we can use the multiplication rule of probability to expand 𝑃(𝑥1∩𝑥2∩𝑥3∩𝑐𝑙𝑎𝑠𝑠)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a8607-db1c-43c6-9a71-b0b761bcb4a3",
   "metadata": {},
   "source": [
    "$$\n",
    "P(x_1 \\cap x_2 \\cap x_3 \\cap \\text{class}) = P(x_1 \\mid x_2 \\cap x_3 \\cap \\text{class}) \\cdot P(x_2 \\cap x_3 \\cap \\text{class})\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1 \\cap x_2 \\cap x_3 \\cap \\text{class}) = P(x_1 \\mid x_2 \\cap x_3 \\cap \\text{class}) \\cdot P(x_2 \\mid x_3 \\cap \\text{class}) \\cdot P(x_3 \\cap \\text{class})\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1 \\cap x_2 \\cap x_3 \\cap \\text{class}) = P(x_1 \\mid x_2 \\cap x_3 \\cap \\text{class}) \\cdot P(x_2 \\mid x_3 \\cap \\text{class}) \\cdot P(x_3 \\mid \\text{class}) \\cdot P(\\text{class})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be481c1d-0df5-4729-832c-1343653171f5",
   "metadata": {},
   "source": [
    "As you can see, the equation is getting pretty long for just three features. Imagine if we would like to train a sentiment classifier based on 10,000+ texts, it would require a lot of calculations. This is where the Naive Bayes's assumption of conditional independence between predictors helps. With the assumption, the formula can be simplified to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a70a8-30a8-4d2c-9fb7-7cfb23d9d81e",
   "metadata": {},
   "source": [
    "$$\n",
    "P(x_1 \\cap x_2 \\cap x_3 \\cap \\text{class}) = P(x_1 \\mid \\text{class}) \\cdot P(x_2 \\mid \\text{class}) \\cdot P(x_3 \\mid \\text{class}) \\cdot P(\\text{class})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf3194-3361-428b-884f-0affd1efb1ba",
   "metadata": {},
   "source": [
    "Hence, the Naive Bayes algorithm can be summarized as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d86db-e68a-45a4-a880-e10d3062f3a2",
   "metadata": {},
   "source": [
    "$$\n",
    "P(\\text{class} \\mid x_1, x_2, x_3) \\propto P(x_1 \\mid \\text{class}) \\cdot P(x_2 \\mid \\text{class}) \\cdot P(x_3 \\mid \\text{class}) \\cdot P(\\text{class})\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{class} \\mid x_1, x_2, \\ldots, x_n) \\propto P(\\text{class}) \\cdot \\prod_{i=1}^n P(x_i \\mid \\text{class})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed71f69b-96cb-4a1e-b278-3ea0d261c9f7",
   "metadata": {},
   "source": [
    "It is good to note that the assumption of independence between predictors is unrealistic in practice. For example, certain words such as \"happy\", \"fantastic\" and \"great\" are more likely to appear together (i.e., dependent). As the assumption rarely holds in practice, the algorithm is called Naïve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dac659-96ec-4b80-a2cb-e94c66f31b39",
   "metadata": {},
   "source": [
    "Despite the simplifying assumption, Naïve Bayes is an easy and fast model that works pretty well in many real-world problems. With the understanding of the algorithm, let's now implement it from scratch to detect hate speech in tweets (sentiment analysis). Note that we shall revisit sentiment analysis using neural networks (deep learning) after some labs, but this labsheet shall focus only on the Naïve Bayes Algorithm taught in class, and what we revisited in the details above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba9ca9-b76e-493e-b227-784c9fbe34e4",
   "metadata": {},
   "source": [
    "## Implementation of Naive Bayes Classification from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3151ec-7c67-478a-a107-49119b615b44",
   "metadata": {},
   "source": [
    "### 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa3a744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.3.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Using cached numpy-2.3.2-cp313-cp313-win_amd64.whl (12.8 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.3.2\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user\\downloads\\naive bayes classification on text data\\lab_3 (naive bayes classification on text data)\\.venv\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\downloads\\naive bayes classification on text data\\lab_3 (naive bayes classification on text data)\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\downloads\\naive bayes classification on text data\\lab_3 (naive bayes classification on text data)\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.2-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.0 MB 1.0 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 1.0/11.0 MB 1.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.6/11.0 MB 1.9 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.6/11.0 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.2/11.0 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.4/11.0 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/11.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 6.8 MB/s  0:00:02\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   ---------------------------------------- 3/3 [pandas]\n",
      "\n",
      "Successfully installed pandas-2.3.2 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5541c31-cb00-418f-bdc1-4b373e710827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d1b60cf-aedc-434f-a505-25fe65b51a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('tweet_sentiment.csv')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d136fe1-1ef1-4695-8743-642f098a7eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    29720\n",
       "1     2242\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename Columns, Drop `id`, and Check Label Counts\n",
    "tweets.rename(columns={'label':'Label', 'tweet':'Tweet'}, inplace=True)  \n",
    "tweets.drop(columns=['id'], inplace=True)    \n",
    "tweets.Label.value_counts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2082819e-3d67-4eb2-b709-2331a6cff011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (5242, 2)\n",
      "Label\n",
      "0    3000\n",
      "1    2242\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filtered Dataset: 3000 Negative & All Positive tweets\n",
    "df_class0 = tweets[tweets['Label'] == 0].sample(n=3000, random_state=42)  # 3000 samples where sentiment == 0\n",
    "df_class1 = tweets[tweets['Label'] == 1]  # All records where sentiment == 1\n",
    "tweets = pd.concat([df_class0, df_class1], axis=0).reset_index(drop=True) # Combine both\n",
    "print(\"Shape:\", tweets.shape)\n",
    "print(tweets['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd65465-a997-4be5-9dbf-57d7bae98321",
   "metadata": {},
   "source": [
    "### 2. Creating Training and Testing Data [1M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f8e635a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\user\\downloads\\naive bayes classification on text data\\lab_3 (naive bayes classification on text data)\\.venv\\lib\\site-packages (from scikit-learn) (2.3.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.7.1-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached scipy-1.16.1-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ---------------------------------------- 4/4 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa81b4cc-be22-4362-a4f4-63c57fad3cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of Train data: 4193\n",
      "Counts of Test data: 1049\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so exo dropped 2 music videos today and i have...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user its father's day, n my father passsed aw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#italians weren't coming from a country were w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am thankful for vacation days. #thankful #po...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ok.  this logres book is getting interesting. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Label\n",
       "0  so exo dropped 2 music videos today and i have...      0\n",
       "1  @user its father's day, n my father passsed aw...      0\n",
       "2  #italians weren't coming from a country were w...      1\n",
       "3  i am thankful for vacation days. #thankful #po...      0\n",
       "4  ok.  this logres book is getting interesting. ...      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test sets with ratio of 80:20\n",
    "# IMPORTANT:\n",
    "# - Preserve label distribution → use stratify\n",
    "# - Use random_state=1 for reproducibility\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tweets['Tweet'], tweets['Label'], test_size= 0.2, stratify= tweets['Label'], random_state=1   # Fill here\n",
    ")\n",
    "\n",
    "# Combine the results into train and test dataframes\n",
    "tweets_train = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "tweets_test = pd.concat([X_test, y_test], axis = 1).reset_index(drop=True)  # Fill here\n",
    "\n",
    "print(\"Counts of Train data:\", len(tweets_train))\n",
    "print(\"Counts of Test data:\", len(tweets_test))\n",
    "\n",
    "# Check a few rows\n",
    "tweets_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b23520-3614-4f09-a38d-633353a438e5",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing [2M]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df064c22-4e30-4a4d-a4cb-d60f3801c22a",
   "metadata": {},
   "source": [
    "We need to first preprocess the tweets to remove hashtags, mentions, punctuations, stopwords, website links, non-alphanumeric characters, single characeter and extra spaces. Here, we'll use nltk built-in TweetTokenizer to tokenize the sentence. It generally works better than the normal nltk word_tokenize function as it splits the sentence by taking into consideration of the common patterns seen in tweets (e.g., mention, hashtags, emoji)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77f202aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\downloads\\naive bayes classification on text data\\lab_3 (naive bayes classification on text data)\\.venv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.7.34-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\downloads\\naive bayes classification on text data\\lab_3 (naive bayes classification on text data)\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.7 MB/s  0:00:00\n",
      "Downloading regex-2025.7.34-cp313-cp313-win_amd64.whl (275 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ---------------------------------------- 4/4 [nltk]\n",
      "\n",
      "Successfully installed click-8.2.1 nltk-3.9.1 regex-2025.7.34 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c4bd74c-b59d-4c59-9528-9ad6b5c6d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from re import sub\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "307e1c1a-3976-43f9-a836-29e8c57f7200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d25ec664-85f0-4eca-ba52-d8a4a0a757db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize objects once (not inside function)\n",
    "twt_tokenizer = TweetTokenizer(strip_handles=True)\n",
    "stops = set(stopwords.words(\"english\")) | set(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Clean and tokenize tweets:\n",
    "    - Remove handles, hashtags, urls\n",
    "    - Lowercase\n",
    "    - Remove stopwords, punctuation, non-alphanumeric, single chars\n",
    "    - Lemmatize tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove URLs before tokenizing\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)  # HINT: use re.sub()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = twt_tokenizer.tokenize(tweet)  # HINT: use twt_tokenizer\n",
    "\n",
    "    # Remove hashtag symbol but keep the word\n",
    "    tokens = [re.sub(r\"#\", \"\", token) for token in tokens]\n",
    "\n",
    "    # Lowercase + remove stopwords & punctuation\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stops]   # HINT: use .lower() and check in stops\n",
    "\n",
    "    # Keep only alphanumeric tokens\n",
    "    tokens = [token for token in tokens if token.isalnum()]   # HINT: use str.isalnum()\n",
    "\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # HINT: use lemmatizer\n",
    "\n",
    "    # Remove single-character tokens\n",
    "    tokens = [t for t in tokens if len(t) > 1]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c627c8e2-901a-4814-8d5b-d0b40f91651c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[exo, dropped, music, video, today, seen, eith...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[day, father, passsed, away, hey]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[italian, coming, country, woman, stoned, ente...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[thankful, vacation, day, thankful, positive]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ok, logres, book, getting, interesting]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Label\n",
       "0  [exo, dropped, music, video, today, seen, eith...      0\n",
       "1                  [day, father, passsed, away, hey]      0\n",
       "2  [italian, coming, country, woman, stoned, ente...      1\n",
       "3      [thankful, vacation, day, thankful, positive]      0\n",
       "4           [ok, logres, book, getting, interesting]      0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply to dataframe column\n",
    "tweets_train[\"Tweet\"] = tweets_train[\"Tweet\"].apply(clean_tweet)\n",
    "tweets_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63acb5fc-1679-40b0-9133-4260d2d42b9d",
   "metadata": {},
   "source": [
    "### 4. Naïve Bayes Model [2M]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40666946-898f-4da9-98e6-dc15fa4f339c",
   "metadata": {},
   "source": [
    "In order to calculate 𝑃(𝑥𝑖|𝑐𝑙𝑎𝑠𝑠), we need to use the following equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdcc8a2-f067-4dc3-bb2d-c7cb152720b4",
   "metadata": {},
   "source": [
    "$$\n",
    "P(x_i \\mid \\text{class}) = \\frac{N_{x_i|\\text{class}} + \\alpha}{N_{\\text{class}} + \\alpha \\cdot N_{\\text{Vocabulary}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74559995-e81f-4824-8378-e20544932c25",
   "metadata": {},
   "source": [
    "- 𝑁𝑥𝑖|𝑐𝑙𝑎𝑠𝑠: total number of a word given the class\n",
    "- 𝑁𝑐𝑙𝑎𝑠𝑠: total number of all the words in the class\n",
    "- 𝑁𝑉𝑜𝑐𝑎𝑏𝑢𝑙𝑎𝑟𝑦: total number of all the words in all classes\n",
    "- 𝛼: Laplace smoothing parameter. This is a non-zero parameter (usually set as 1) to tackle the zero-probability problem in Naive Bayes algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c882d-af33-409f-8b62-2d06da1cc755",
   "metadata": {},
   "source": [
    "#### 4.1 Create Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa33e7-b8e9-4dce-a62c-1c302e6ea8b4",
   "metadata": {},
   "source": [
    "Let's create the vocabulary, which includes all the unique words in the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7ab3099-3a18-4c21-a1a5-6fbd44784deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocabularies in the training set is : 9718\n"
     ]
    }
   ],
   "source": [
    "# Create Vacabulary\n",
    "vocab = []\n",
    "for index, value in tweets_train['Tweet'].items():\n",
    "    for word in value:\n",
    "        vocab.append(word)\n",
    "        \n",
    "vocab = list(set(vocab))  # remove duplicate words\n",
    "print(\"Number of vocabularies in the training set is : {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5cf7b-9d0d-45c7-9529-e2cd5d79a187",
   "metadata": {},
   "source": [
    "#### 4.2 Vocabulary–Tweet Occurrence Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4da88-88d0-438b-aa01-64d43efaea12",
   "metadata": {},
   "source": [
    "Next, we need to transform the training dataset to include columns of vocabularies and the occurrence of each vocabulary in each tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00ad87f9-5f94-4355-ba16-b73290553876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 31076 stored elements and shape (4193, 9718)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# TASK:\n",
    "# - Combine all tweets in training set into a single list of strings\n",
    "# - Use CountVectorizer to convert tweets into Bag-of-Words representation\n",
    "\n",
    "# TODO: Combine all the tweets into a single list\n",
    "corpus = tweets_train['Tweet'].apply(lambda x: \" \".join(x) )\n",
    "\n",
    "# TODO: Initialize and fit CountVectorizer on the corpus\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_wc = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Display transformed training data\n",
    "X_train_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56187a8d-c800-43e8-b47e-424fcfb3b839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "      <th>05068231</th>\n",
       "      <th>0608</th>\n",
       "      <th>0612</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>...</th>\n",
       "      <th>ªã</th>\n",
       "      <th>ªé</th>\n",
       "      <th>ªð</th>\n",
       "      <th>µï</th>\n",
       "      <th>µð</th>\n",
       "      <th>ºâ</th>\n",
       "      <th>ºéº</th>\n",
       "      <th>ºï</th>\n",
       "      <th>ºð</th>\n",
       "      <th>êµ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[exo, dropped, music, video, today, seen, eith...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[day, father, passsed, away, hey]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[italian, coming, country, woman, stoned, ente...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[thankful, vacation, day, thankful, positive]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ok, logres, book, getting, interesting]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9720 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Label  05068231  0608  \\\n",
       "0  [exo, dropped, music, video, today, seen, eith...      0         0     0   \n",
       "1                  [day, father, passsed, away, hey]      0         0     0   \n",
       "2  [italian, coming, country, woman, stoned, ente...      1         0     0   \n",
       "3      [thankful, vacation, day, thankful, positive]      0         0     0   \n",
       "4           [ok, logres, book, getting, interesting]      0         0     0   \n",
       "\n",
       "   0612  08  09  10  100  1000  ...  ªã  ªé  ªð  µï  µð  ºâ  ºéº  ºï  ºð  êµ  \n",
       "0     0   0   0   0    0     0  ...   0   0   0   0   0   0    0   0   0   0  \n",
       "1     0   0   0   0    0     0  ...   0   0   0   0   0   0    0   0   0   0  \n",
       "2     0   0   0   0    0     0  ...   0   0   0   0   0   0    0   0   0   0  \n",
       "3     0   0   0   0    0     0  ...   0   0   0   0   0   0    0   0   0   0  \n",
       "4     0   0   0   0    0     0  ...   0   0   0   0   0   0    0   0   0   0  \n",
       "\n",
       "[5 rows x 9720 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_counts = pd.DataFrame(X_train_wc.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tweets_train = pd.concat([tweets_train, vocab_counts], axis=1)  # Concatenate vocab_counts with the original dataframe\n",
    "tweets_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aafa57-d7b4-4b74-81a6-d78d592a14c2",
   "metadata": {},
   "source": [
    "#### 4.3 Calculate Priors and Conditional Probabilities [2M]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33922dd7-dd8f-46c5-ac6a-266ff4b73dcf",
   "metadata": {},
   "source": [
    "Next, let's calculate total number of words in each class. Then, we can calculate the conditional probability of occurrence of each word given the class and use them to predict whether a tweet contains hate speech. Recollect the formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc73c43-5242-47fb-a681-01aaaf2b6394",
   "metadata": {},
   "source": [
    "$$\n",
    "P(x_i \\mid \\text{class}) = \\frac{N_{x_i|\\text{class}} + \\alpha}{N_{\\text{class}} + \\alpha \\cdot N_{\\text{Vocabulary}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "246324bf-4724-45b6-919c-2677a16b8c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(hate) = 0.42761745766754117 , P(non-hate) = 0.5723825423324589\n"
     ]
    }
   ],
   "source": [
    "# Calculate Prior Probabilities\n",
    "\n",
    "# TASK:\n",
    "# Calculate prior probabilities:\n",
    "# P(hate)   = (# of hate tweets / total tweets)\n",
    "# P(non-hate) = (# of non-hate tweets / total tweets)\n",
    "\n",
    "# TODO: Fill in the missing code to calculate class priors\n",
    "p_hate    = tweets_train['Label'].value_counts(normalize=True)[1]   # HINT: use 1 for hate\n",
    "p_nonhate = tweets_train['Label'].value_counts(normalize=True)[0]   # HINT: use 0 for non-hate\n",
    "\n",
    "print(\"P(hate) =\", p_hate, \", P(non-hate) =\", p_nonhate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "654fe3fc-ab9c-414a-8158-32122603c573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_hate = 13971 , N_nonhate = 18149 , Vocab size = 9718\n"
     ]
    }
   ],
   "source": [
    "# Word Counts & Vocabulary Size\n",
    "\n",
    "alpha = 1  # Set Laplace smoothing parameter (to avoid zero probabilities)\n",
    "\n",
    "# TASK:\n",
    "# - Count total number of words in hate tweets (N_hate)\n",
    "# - Count total number of words in non-hate tweets (N_nonhate)\n",
    "# - Get vocabulary size (N_vocab)\n",
    "\n",
    "# TODO: Calculate N_hate (total words in hate class)\n",
    "n_hate = tweets_train.loc[tweets_train['Label'] == 1, 'Tweet'].apply(len).sum()\n",
    "\n",
    "# TODO: Calculate N_nonhate (total words in non-hate class)\n",
    "n_nonhate = tweets_train.loc[tweets_train['Label'] == 0, 'Tweet'].apply(len).sum()\n",
    "\n",
    "# TODO: Size of vocabulary\n",
    "n_vocab = len(vocab)\n",
    "\n",
    "print(\"N_hate =\", n_hate, \", N_nonhate =\", n_nonhate, \", Vocab size =\", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "022c7cf2-468d-423e-9a6d-c6c6faa1c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Probabilities\n",
    "\n",
    "# TASK:\n",
    "# For each word in vocabulary, calculate:\n",
    "# P(word | hate)     = (count(word in hate) + alpha) / (N_hate + alpha * V)\n",
    "# P(word | non-hate) = (count(word in non-hate) + alpha) / (N_nonhate + alpha * V)\n",
    "\n",
    "# Initialize dictionaries\n",
    "parameters_hate    = {word: 0 for word in vocab}\n",
    "parameters_nonhate = {word: 0 for word in vocab}\n",
    "\n",
    "# TODO: Calculate conditional probabilities\n",
    "for word in vocab:\n",
    "    n_word_given_hate    = tweets_train.loc[tweets_train['Label'] == 1, word].sum()\n",
    "    n_word_given_nonhate = tweets_train.loc[tweets_train['Label'] == 0, word].sum()\n",
    "\n",
    "    parameters_hate[word]    = (n_word_given_hate + alpha) / (n_hate + alpha * n_vocab)\n",
    "    parameters_nonhate[word] = (n_word_given_nonhate + alpha) / (n_nonhate + alpha * n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f0298-ecd2-43b3-a8ea-f156aaa5a19d",
   "metadata": {},
   "source": [
    "### 5. Classifying a New Tweet and Evaluation [2M]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daee8a5-f24c-4ce0-97f4-74affdd41bdb",
   "metadata": {},
   "source": [
    "Now, we have all the components ready to build the classifier for hate speech detection. The steps of sentiment analysis are:\n",
    "- Take in a new tweet (𝑥1, 𝑥2, … , 𝑥𝑛) as input\n",
    "- Calculate 𝑃(ℎ𝑎𝑡𝑒|𝑥1, 𝑥2, … , 𝑥𝑛) and 𝑃(𝑛𝑜𝑛_ℎ𝑎𝑡𝑒|𝑥1, 𝑥2, … , 𝑥𝑛)\n",
    "- Compare 𝑃(ℎ𝑎𝑡𝑒|𝑥1, 𝑥2, … , 𝑥𝑛) and 𝑃(𝑛𝑜𝑛_ℎ𝑎𝑡𝑒|𝑥1, 𝑥2, … , 𝑥𝑛)\n",
    "  - If 𝑃(ℎ𝑎𝑡𝑒|𝑥1, 𝑥2, … , 𝑥𝑛) > 𝑃(𝑛𝑜𝑛_ℎ𝑎𝑡𝑒|𝑥1, 𝑥2, … , 𝑥𝑛), tweet is classified as containing hate speech\n",
    "  - If 𝑃(ℎ𝑎𝑡𝑒|𝑥1, 𝑥2, … , 𝑥𝑛) < 𝑃(𝑛𝑜𝑛_ℎ𝑎𝑡𝑒|𝑥1, 𝑥2, … , 𝑥𝑛), tweet is classified as not containing hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d9af238-a670-4235-b8fc-ee02a9808bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "      <th>Predicted_scratch</th>\n",
       "      <th>Correct_scratch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>another  comment on social media!! idiot!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to all of those, who answer  replies to my you...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user #patriarchal, #misogynist,  cult. #chris...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stop bullying. stop racism. everyone can stand...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user #allahsoil we should not conflate islami...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Label  \\\n",
       "0       another  comment on social media!! idiot!!!       1   \n",
       "1  to all of those, who answer  replies to my you...      1   \n",
       "2  @user #patriarchal, #misogynist,  cult. #chris...      1   \n",
       "3  stop bullying. stop racism. everyone can stand...      1   \n",
       "4  @user #allahsoil we should not conflate islami...      1   \n",
       "\n",
       "   Predicted_scratch  Correct_scratch  \n",
       "0                  1             True  \n",
       "1                  0            False  \n",
       "2                  1             True  \n",
       "3                  1             True  \n",
       "4                  1             True  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK:\n",
    "# - Preprocess the input tweet\n",
    "# - Start with prior probabilities (p_hate, p_nonhate)\n",
    "# - For each word, multiply conditional probabilities if word exists in training\n",
    "# - Return predicted class (1 = hate, 0 = non-hate)\n",
    "\n",
    "def classify_new_tweet(tweet):\n",
    "    # TODO: Preprocess new tweet\n",
    "    words = clean_tweet(tweet)    # HINT: use clean_tweet\n",
    "    \n",
    "    # TODO: Initialize with priors\n",
    "    p_hate_given_words    = p_hate\n",
    "    p_nonhate_given_words = p_nonhate\n",
    "    \n",
    "    # Loop through each word in the tweet\n",
    "    for word in words:\n",
    "        # Multiply conditional probabilities if word seen in training vocab\n",
    "        if word in parameters_hate:\n",
    "            p_hate_given_words *= parameters_hate[word]\n",
    "                \n",
    "        if word in parameters_nonhate:\n",
    "            p_nonhate_given_words *= parameters_nonhate[word]\n",
    "    \n",
    "    # TODO: Choose class with higher probability\n",
    "    if p_hate_given_words > p_nonhate_given_words:\n",
    "        return 1   # hate\n",
    "    else:\n",
    "        return 0   # non-hate\n",
    "\n",
    "\n",
    "# Apply classifier on test data\n",
    "tweets_test['Predicted_scratch'] = tweets_test['Tweet'].apply( classify_new_tweet )\n",
    "tweets_test['Correct_scratch']   = (tweets_test['Label'] == tweets_test['Predicted_scratch'])\n",
    "\n",
    "# Preview results\n",
    "tweets_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0494ad6-4fba-4031-aeb2-e4012627f41d",
   "metadata": {},
   "source": [
    "### Evaluate the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "feb51e9e-d24e-4bf1-98af-42925e24914f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratch NB Accuracy: 0.9008579599618685\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy of scratch NB\n",
    "accuracy_scratch = tweets_test['Correct_scratch'].mean()\n",
    "print(\"Scratch NB Accuracy:\", accuracy_scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806ea27-76d7-4572-bec7-8ce8f7e486e2",
   "metadata": {},
   "source": [
    "## Implementation Using SKlearn [1M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4793be0c-743f-4ef1-8cf3-8beb05d549bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "875a7683-92c0-4303-b724-0413dc9191ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train and y_test to numpy array\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d51cac6-dca2-4533-b0cd-77dc9bab8775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (4193, 9718)\n",
      "Shape of y_train: (4193,)\n",
      "Shape of X_test: (1049, 9718)\n",
      "Shape of y_test: (1049,)\n"
     ]
    }
   ],
   "source": [
    "# Clean + join test tweets into a string corpus\n",
    "corpus_test = tweets_test['Tweet'].apply(clean_tweet).apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Use the same vectorizer as training to transform test data\n",
    "X_test_wc = vectorizer.transform(corpus_test)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Shape of X_train: {}\".format(X_train_wc.shape))\n",
    "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
    "print(\"Shape of X_test: {}\".format(X_test_wc.shape))\n",
    "print(\"Shape of y_test: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a5e893b-1c9c-4d88-b0bf-0e990bc7a83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultinomialNB</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('alpha',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">alpha&nbsp;</td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('force_alpha',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">force_alpha&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fit_prior',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">fit_prior&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_prior',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_prior&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Naive Bayes classifier\n",
    "clf = MultinomialNB(alpha=1.0)   # Laplace smoothing by default\n",
    "clf.fit(X_train_wc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97dd2fd1-015a-4c72-8dd1-d426372e3662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9008579599618685\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on test set\n",
    "accuracy = clf.score(X_test_wc, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "135203c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.5-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.1-cp313-cp313-win_amd64.whl.metadata (111 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp313-cp313-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\user\\downloads\\naive bayes classification on text data\\lab_3 (naive bayes classification on text data)\\.venv\\lib\\site-packages (from matplotlib) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\downloads\\naive bayes classification on text data\\lab_3 (naive bayes classification on text data)\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.3.0-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\downloads\\naive bayes classification on text data\\lab_3 (naive bayes classification on text data)\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\downloads\\naive bayes classification on text data\\lab_3 (naive bayes classification on text data)\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached matplotlib-3.10.5-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.3-cp313-cp313-win_amd64.whl (226 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.1-cp313-cp313-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.8/2.3 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 7.9 MB/s  0:00:00\n",
      "Using cached kiwisolver-1.4.9-cp313-cp313-win_amd64.whl (73 kB)\n",
      "Using cached pillow-11.3.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ---------------------------------------- 0/7 [pyparsing]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ---------------------------- ----------- 5/7 [contourpy]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------------- 7/7 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.1 kiwisolver-1.4.9 matplotlib-3.10.5 pillow-11.3.0 pyparsing-3.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8e72a8a-5288-45e7-969e-5b477ed3c6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x228227f8f50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusiion Matrix\n",
    "y_pred = clf.predict(X_test_wc)  # Predict labels on the test set\n",
    "cm = confusion_matrix(y_test, y_pred)  # Generate confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2a235ea-b270-493b-8b77-b35dcd1a41a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       600\n",
      "           1       0.89      0.88      0.88       449\n",
      "\n",
      "    accuracy                           0.90      1049\n",
      "   macro avg       0.90      0.90      0.90      1049\n",
      "weighted avg       0.90      0.90      0.90      1049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred, target_names=clf.classes_.astype(str))\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
