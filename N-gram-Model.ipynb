{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b401d711-3bb3-4ebb-a20e-f0760d2ca131",
   "metadata": {},
   "source": [
    "**Lab details:**\n",
    "- Natural language Processing (NLP) - CS429\n",
    "- Lab Session-2\n",
    "- Date- 19/08/2025\n",
    "- Marks - 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027527e-d233-46e5-a394-7de142bbe793",
   "metadata": {},
   "source": [
    "**Instruction:**\n",
    "1) The name of your Python file must follow this format: **studentID_Name**\n",
    "   - Example: If Student ID = 2022B3A70617P and Name = Nikhil Manvendra Singh\n",
    "   - File name should be: 2022B3A70617P_Nikhil_Manvendra_Singh.ipynb\n",
    "2) Submission Deadline: 6:15 PM (Today)\n",
    "3) Students are required to fill the blanks with code syntax in the section provided below and Attempt evaluative question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e3c294-1fc5-4de4-b5b8-f9f18592ff5a",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------Lab Section--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41019ceb-9deb-4779-ab2e-9cca93062a8f",
   "metadata": {},
   "source": [
    "**What is N- Gram Model ?**\n",
    "- N-gram models estimate the probability of the next word in a sequence using a fixed-length history of previous words.\n",
    "- In a bigram model (n=2), the probability of the next word depends on the previous one word.\n",
    "- In a trigram model (n=3), the prediction is based on the previous two words.\n",
    "- In a general n-gram model, the probability of the next word is estimated based on the previous n−1 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a0384-a918-4531-8143-a25a9b1353a2",
   "metadata": {},
   "source": [
    "$$\n",
    "    P(W_n \\mid W_{n-2}, W_{n-1}) = \\frac{C(W_{n-2}, W_{n-1}, W_n)}{C(W_{n-2}, W_{n-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e119d4c-7970-466c-89dc-fd12e33e4455",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "- Let’s work through an example using a mini-corpus of three sentences.\n",
    "- -`<s> I am Sam </s>`\n",
    "- -`<s> Sam I am </s>`\n",
    "- -`<s> I do not like green eggs and ham </s>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d3ca0-0cd4-4d04-9a04-b592c32636cb",
   "metadata": {},
   "source": [
    "**Bigram Probability Calculations:-**\n",
    "- `P(I|<s>) = 2/3 = 6.7`\n",
    "- `P(Sam|<s>) = 1/3 = 0.33`\n",
    "- `P(am|I) = 2/3 = 0.67`\n",
    "- `P(</s>|Sam) = 1/2 = 0.5`\n",
    "- `P(Sam|am) = 1/2 = 0.5 `\n",
    "- `P(do|I) = 1/3 = 0.33`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d512e7-e7d0-4990-bcc2-83100e542924",
   "metadata": {},
   "source": [
    "### Install Below Packages"
   ]
  },
  {
   "cell_type": "raw",
   "id": "386633e7-70d9-46b6-9210-11ab73ef7ad7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!pip install -U pip\n",
    "!pip install --upgrade nltk\n",
    "!pip install -U dill "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "084c17a7-1138-4c9b-be6e-17d09510b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence \n",
    "from nltk.util import bigrams \n",
    "from nltk.util import ngrams \n",
    "from nltk.util import everygrams \n",
    "from nltk.lm.preprocessing import pad_both_ends \n",
    "from nltk.lm.preprocessing import flatten "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432880f6-39d1-404c-b47d-9a7377d9452a",
   "metadata": {},
   "source": [
    "### Bigram and Tri Generation using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f511e4-a4af-4f38-a3be-72b7be6d9419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b'), ('b', 'c')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']] \n",
    "list(bigrams(text[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa48be0e-3320-4429-b489-e99862304f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'c'), ('c', 'd'), ('d', 'c'), ('c', 'e'), ('e', 'f')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(text[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ea06c5f-fec9-46c8-8f89-c2d3f8ba40c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'c', 'd'), ('c', 'd', 'c'), ('d', 'c', 'e'), ('c', 'e', 'f')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(text[1], n=3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b4ab8ad-5620-4888-893a-4b7085fb0563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'c', 'd', 'c'), ('c', 'd', 'c', 'e'), ('d', 'c', 'e', 'f')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write Code to generate 4-gram \n",
    "\n",
    "list(ngrams(text[0], n=4))\n",
    "list(ngrams(text[1], n=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db5235d-0b0b-4012-9fcf-9ae53f1d40f6",
   "metadata": {},
   "source": [
    "Note:-\n",
    "When we build n-grams for NLP, the problem is that the first and last tokens don’t get full context.\n",
    "For example, in ['a', 'b', 'c']:\n",
    "- 'a' only appears in ('a','b') (missing left context).\n",
    "- 'c' only appears in ('b','c') (missing right context).\n",
    "- To fix this, we add padding symbols at the start/end of the sequence\n",
    "- NLTK also has a function for that, let's see what it does to the first sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debbdbcd-995b-4f77-8e04-f9c5f92d9961",
   "metadata": {},
   "source": [
    "### Sequence Padding with Start and End Symbols in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caec3e52-469c-4e2f-a3a4-c2b108391c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'a', 'b', 'c', '</s>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import pad_sequence \n",
    "list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=2)) # The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1887636-e7c3-48e3-840e-2e337756a445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'a'), ('a', 'b'), ('b', 'c'), ('c', '</s>')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sent = list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=2)) \n",
    "list(ngrams(padded_sent, n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2f844a4-366c-4224-bc59-2cad4cd7a3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<s>', 'a', 'b', 'c', '</s>', '</s>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pad_sequence(text[0],pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=3)) # The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a697800-108d-4971-b46f-8852e86c93ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', 'a'),\n",
       " ('<s>', 'a', 'b'),\n",
       " ('a', 'b', 'c'),\n",
       " ('b', 'c', '</s>'),\n",
       " ('c', '</s>', '</s>')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sent = list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=3))  \n",
    "list(ngrams(padded_sent, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf116b0b-56be-4cb9-bc35-e38932300b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', '<s>', 'a'),\n",
       " ('<s>', '<s>', 'a', 'b'),\n",
       " ('<s>', 'a', 'b', 'c'),\n",
       " ('a', 'b', 'c', '<s>'),\n",
       " ('b', 'c', '<s>', '<s>'),\n",
       " ('c', '<s>', '<s>', '<s>')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write Code to generate 4-gram fot second list of 'text'\n",
    "padded_sent = list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"<s>\", n=4))\n",
    "list(ngrams(padded_sent, n=4))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b085e8ef-8bed-48a0-9899-28e435932d8d",
   "metadata": {},
   "source": [
    "Note:- \n",
    "- Here the n argument, that tells the function we need padding for N-grams. \n",
    "- Now, passing all these parameters every time is tedious and in most cases they can be safely assumed as defaults anyway. \n",
    "- Thus the nltk.lm module provides a convenience function that has all these arguments already set while the other arguments remain the same as for pad_sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c9f8086-4eff-43be-a92a-3c52b7fcadb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'a', 'b', 'c', '</s>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends \n",
    "list(pad_both_ends(text[0], n=2))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cc49d8a-6be0-49d2-aa95-2b2e6e237e82",
   "metadata": {},
   "source": [
    "Combining the two parts discussed so far we get the following preparation steps for one sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a85fec5-d6b2-42e0-a7eb-d27dd45572f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'a'), ('a', 'b'), ('b', 'c'), ('c', '</s>')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(pad_both_ends(text[0], n=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088978d9-9369-4bd8-af56-5070ba1a69ca",
   "metadata": {},
   "source": [
    "### Every gram generation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae0d1688-70a1-4cfa-995c-a002bf27f7d7",
   "metadata": {},
   "source": [
    "To make our model more robust we could also train it on unigrams (single words) as well as bigrams, its main source of information. \n",
    "- NLTK once again helpfully provides a function called everygrams.\n",
    "- While not the most efficient, it is conceptually simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de2b455d-90db-48b5-807c-dc8cbcb20eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>', 'a'),\n",
       " ('a',),\n",
       " ('a', 'b'),\n",
       " ('b',),\n",
       " ('b', 'c'),\n",
       " ('c',),\n",
       " ('c', '</s>'),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import everygrams \n",
    "padded_bigrams = list(pad_both_ends(text[0], n=2)) \n",
    "list(everygrams(padded_bigrams, max_len=2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "779649a8-eb6c-4f1d-aded-48c19ead7376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>', '<s>'),\n",
       " ('<s>', '<s>', 'a'),\n",
       " ('<s>',),\n",
       " ('<s>', 'a'),\n",
       " ('<s>', 'a', 'b'),\n",
       " ('a',),\n",
       " ('a', 'b'),\n",
       " ('a', 'b', 'c'),\n",
       " ('b',),\n",
       " ('b', 'c'),\n",
       " ('b', 'c', '</s>'),\n",
       " ('c',),\n",
       " ('c', '</s>'),\n",
       " ('c', '</s>', '</s>'),\n",
       " ('</s>',),\n",
       " ('</s>', '</s>'),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Every gram model for sencond list of 'text' for n = 3\n",
    "padded_bigrams = list(pad_both_ends(text[0], n=3)) \n",
    "list(everygrams(padded_bigrams, max_len=3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02df22c2-f6de-4914-83db-1e1a5c7b69d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>', '<s>'),\n",
       " ('<s>', '<s>', '<s>'),\n",
       " ('<s>', '<s>', '<s>', 'a'),\n",
       " ('<s>',),\n",
       " ('<s>', '<s>'),\n",
       " ('<s>', '<s>', 'a'),\n",
       " ('<s>', '<s>', 'a', 'b'),\n",
       " ('<s>',),\n",
       " ('<s>', 'a'),\n",
       " ('<s>', 'a', 'b'),\n",
       " ('<s>', 'a', 'b', 'c'),\n",
       " ('a',),\n",
       " ('a', 'b'),\n",
       " ('a', 'b', 'c'),\n",
       " ('a', 'b', 'c', '</s>'),\n",
       " ('b',),\n",
       " ('b', 'c'),\n",
       " ('b', 'c', '</s>'),\n",
       " ('b', 'c', '</s>', '</s>'),\n",
       " ('c',),\n",
       " ('c', '</s>'),\n",
       " ('c', '</s>', '</s>'),\n",
       " ('c', '</s>', '</s>', '</s>'),\n",
       " ('</s>',),\n",
       " ('</s>', '</s>'),\n",
       " ('</s>', '</s>', '</s>'),\n",
       " ('</s>',),\n",
       " ('</s>', '</s>'),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Every gram model for sencond list of 'text' for n = 4\n",
    "padded_bigrams = list(pad_both_ends(text[0], n=4)) \n",
    "list(everygrams(padded_bigrams, max_len=4)) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3bdaf79-7503-4588-b6fc-2251f8c215a6",
   "metadata": {},
   "source": [
    "- We are almost ready to start counting ngrams, just one more step left. \n",
    "- During training and evaluation our model will rely on a vocabulary that defines which words are \"known\" to the model. \n",
    "- To create this vocabulary, we need to pad our sentences (just like for counting ngrams) and then combine the sentences into one flat stream of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fa325b9-a64c-4bb9-a62d-e23f14e3ba1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'a', 'b', 'c', '</s>', '<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import flatten \n",
    "list(flatten(pad_both_ends(sent, n=2) for sent in text))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d263489-0117-4f37-a489-809bb460bc2d",
   "metadata": {},
   "source": [
    "- In most cases we want to use the same text as the source for both vocabulary and n-gram counts.\n",
    "- Now that we understand what this means for our preprocessing, we can simply import a function that does everything for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15af887e-7bd3-4709-862a-2dae3f2fe612",
   "metadata": {},
   "source": [
    "### Generates the training data and vocabulary set with padding using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172dc6d-9348-40a6-8db3-29acd64b8a84",
   "metadata": {},
   "source": [
    "**padded_everygram_pipeline(n, tokenized_text):** \n",
    "- This function prepares data for training an n-gram language model by:\n",
    "1) Padding sentences → adds start (`<s>`) and end (`</s>`) tokens.\n",
    "2) Generating n-grams → creates all possible n-grams (e.g., bigrams, trigrams) from the padded sentences.\n",
    "\n",
    "- Returns two lazy iterators:\n",
    "1) training_ngrams → iterator over n-grams for each sentence.\n",
    "2) padded_sentences → iterator over the padded sentences (used for building the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "860bc59f-79b7-4fd7-8d35-09f068478061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline \n",
    "train, vocab = padded_everygram_pipeline(2, text) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ee68568-7556-4171-be67-1033c24f92e3",
   "metadata": {},
   "source": [
    "- So as to avoid re-creating the text in memory, both train and vocab are lazy iterators.\n",
    "- They are evaluated on demand at training time. \n",
    "- For the sake of understanding the output of padded_everygram_pipeline, we'll \"materialize\" the lazy iterators by casting them into a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f3d9b00-3fd5-4477-aaad-09970d92391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>',), ('<s>', 'a'), ('a',), ('a', 'b'), ('b',), ('b', 'c'), ('c',), ('c', '</s>'), ('</s>',)]\n",
      "\n",
      "#############\n",
      "[('<s>',), ('<s>', 'a'), ('a',), ('a', 'c'), ('c',), ('c', 'd'), ('d',), ('d', 'c'), ('c',), ('c', 'e'), ('e',), ('e', 'f'), ('f',), ('f', '</s>'), ('</s>',)]\n",
      "\n",
      "#############\n"
     ]
    }
   ],
   "source": [
    "training_ngrams, padded_sentences = padded_everygram_pipeline(2, text) \n",
    "for ngramlize_sent in training_ngrams:\n",
    "    print(list(ngramlize_sent))\n",
    "    print()\n",
    "    print('#############')\n",
    "    list(padded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e4235-feea-4554-9619-0348f7cc4fcd",
   "metadata": {},
   "source": [
    "### Real data and tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b5d62f1-8f74-4710-a27f-66efe5c5433a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language',\n",
       " 'is',\n",
       " 'never',\n",
       " ',',\n",
       " 'ever',\n",
       " ',',\n",
       " 'ever',\n",
       " ',',\n",
       " 'random',\n",
       " 'adam',\n",
       " 'kilgarriff',\n",
       " 'abstract',\n",
       " 'language',\n",
       " 'users',\n",
       " 'never',\n",
       " 'choose',\n",
       " 'words',\n",
       " 'randomly',\n",
       " ',',\n",
       " 'and',\n",
       " 'language',\n",
       " 'is',\n",
       " 'essentially',\n",
       " 'non-random',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try: # Use the default NLTK tokenizer. \n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    # Testing whether it works. \n",
    "    # Sometimes it doesn't work on some machines because of setup issues. \n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0]) \n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer \n",
    "    # See https://stackoverflow.com/a/25736515/610569 \n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x) \n",
    "    # Use the toktok tokenizer that requires no dependencies. \n",
    "    toktok = ToktokTokenizer() \n",
    "    word_tokenize = word_tokenize = toktok.tokenize \n",
    "\n",
    "\n",
    "import os \n",
    "import requests\n",
    "import io #codecs \n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline \n",
    "train, vocab = padded_everygram_pipeline(2, text) \n",
    " \n",
    " \n",
    "# Text version of https://kilgarriff.co.uk/Publications/2005-K- lineer.pdf \n",
    "if os.path.isfile('language-never-random.txt'):\n",
    "    with io.open('language-never-random.txt', encoding='utf8') as fin:\n",
    "        text = fin.read() \n",
    "else:\n",
    "    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\" \n",
    "    text = requests.get(url).content.decode('utf8') \n",
    "    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
    "        fout.write(text) \n",
    "        \n",
    "# Tokenize the text. \n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent)))\n",
    "                  for sent in sent_tokenize(text)] \n",
    "tokenized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8794d3f3-2514-4e11-9ed3-c9a0dc3e00c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Language is never, ever, ever, random\n",
      "\n",
      "                                                               ADAM KILGARRIFF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Abstract\n",
      "Language users never choose words randomly, and language is essentially\n",
      "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
      "posits randomness. Hence, when we look at linguistic phenomena in cor-\n",
      "pora, the null hypothesis will never be true. Moreover, where there is enough\n",
      "data, we shall (almost) always be able to establish \n"
     ]
    }
   ],
   "source": [
    "print(text[:500]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45a21cb7-e900-413b-b676-e63bea8f35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for 3-grams language modelling \n",
    "n = 3 \n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4b04cc5-8b5e-4d82-bfcf-f926c5cb49ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object padded_everygram_pipeline.<locals>.<genexpr> at 0x000001F139A62B30>\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e31f5a98-fd35-44b5-a570-eb50dbe66f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<itertools.chain object at 0x000001F139A39DF0>\n"
     ]
    }
   ],
   "source": [
    "print(padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9a3d304-3e15-4c39-a8bc-c241ff22b0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object padded_everygram_pipeline.<locals>.<genexpr> at 0x000001F139D484A0>\n",
      "<itertools.chain object at 0x000001F139A7F370>\n"
     ]
    }
   ],
   "source": [
    "# prepare training and padded sequence for 4-Gram Language Modeling\n",
    "n = 4\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "print(train_data)\n",
    "print(padded_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa8611d-1896-415f-8132-ec2796af2d8a",
   "metadata": {},
   "source": [
    "### Training an N-gram Language Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f6449ba-d663-456b-8606-fe72bc09f2d0",
   "metadata": {},
   "source": [
    "- Having prepared our data we are ready to start training a model. \n",
    "- As a simple example, let us train a Maximum Likelihood Estimator (MLE). \n",
    "- We only need to specify the highest n-gram order to instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34ef1f7b-46c7-4062-960f-7e16373282b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE \n",
    "model = MLE(3) # Lets train a 3-grams model, previously we set n=3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "505c2149-9727-42eb-a12d-347f9701e1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb47b4a0-215d-4a76-8f55-1cd9409233fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 1419 items>\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, padded_sents) \n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2963bfc7-dd17-4ab4-b167-9f93fcf73940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1419"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1434964-5f47-4177-b0c8-c490cc3e8d78",
   "metadata": {},
   "source": [
    "The vocabulary helps us handle words that have not occurred during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ced71f92-c168-4207-8654-694160f27a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.')\n"
     ]
    }
   ],
   "source": [
    "print(model.vocab.lookup(tokenized_text[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "390ef002-b581-4d76-8675-d5a4bef89879",
   "metadata": {},
   "source": [
    "- Moreover, in some cases we want to ignore words that we did see during training but that didn't occur frequently enough, to provide us useful information.\n",
    "- You can tell the vocabulary to ignore such words using the unk_cutoff argument for the vocabulary lookup, To find out how that works, check out the docs for the nltk.lm.vocabulary.Vocabulary class"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d71866cd-bb39-4897-b742-d1dc67032833",
   "metadata": {},
   "source": [
    "Note:  these objects from nltk.lm.models:\n",
    "• Lidstone: Provides Lidstone-smoothed scores. \n",
    "• Laplace: Implements Laplace (add one) smoothing. \n",
    "• InterpolatedLanguageModel: Logic common to all interpolated language models (Chen & Goodman 1995). \n",
    "• WittenBellInterpolated: Interpolated version of Witten-Bell smoothing."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce3fed04-9e36-4b43-8ad0-2bbf187a53d1",
   "metadata": {},
   "source": [
    "The words which we have ignored are known as UNK words that are used to handle unknown words at the inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e1b39-ef3e-44c9-a087-1e951e890ecb",
   "metadata": {},
   "source": [
    "###  N-Gram Language Model: Counts, Probabilities, and Unknown Words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "370e4862-3a16-4c89-a192-de8ee1c53c7e",
   "metadata": {},
   "source": [
    "When it comes to n-gram models the training boils down to counting up the ngrams from the raining corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f069e345-58b5-41c9-af8e-d99a84f672df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 4 ngram orders and 25918 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37c4455e-43b2-4193-ba93-7574295fdd61",
   "metadata": {},
   "source": [
    "This provides a convenient interface to access counts for unigrams... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5719681d-3a35-42b9-a0b2-861a5a72d03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.counts['language'] # i.e. Count('language') "
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfb5dc2f-dfc1-4044-8b06-a75643f92d64",
   "metadata": {},
   "source": [
    "...and bigrams for the phrase \"language is\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b1c1170-8c6e-45d4-a018-2c9be71700f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.counts[['language']]['is'] # i.e. Count('is'|'language') "
   ]
  },
  {
   "cell_type": "raw",
   "id": "535cc158-f5ec-467f-a790-cb96c038da06",
   "metadata": {},
   "source": [
    ".. and trigrams for the phrase \"language is never\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b90a6a9d-7610-457f-bdd7-c3202b1a75ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.counts[['language', 'is']]['never'] # i.e. Count('never'|'language is') "
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb603956-1a2e-4f11-8170-808d0445882e",
   "metadata": {},
   "source": [
    "And so on. However, the real purpose of training a language model is to have it score how probable words are in certain contexts. \n",
    "This being MLE, the model returns the item's relative frequency as its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28f1d7d7-0a1f-4631-9c18-fb13508d72eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003724672228843862"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('language') # P('language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "742dfbdd-1c51-46a2-8dda-7b3675046015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('is', 'language'.split())  # P('is'|'language') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c75dd67-ad1c-4eef-9585-c2812e81e2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363636363636364"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('never', 'language is'.split()) # P('never'|'language is')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e393fb8a-5eda-4493-adf7-6915f0b72050",
   "metadata": {},
   "source": [
    "# Items that are not seen during training are mapped to the vocabulary's \"unknown label\" token. This is \"\" by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57bcecce-67a8-442b-82af-70bf0a614e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(\"<UNK>\") == model.score(\"lah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc24411a-9a6e-406d-b7e8-e7ba64a01f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(\"<UNK>\") == model.score(\"leh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74cc1a1d-0a88-47c9-812a-a1a1baa466ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(\"<UNK>\") == model.score(\"lor\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b26c84e1-e61e-48a4-9038-85f4f4793a17",
   "metadata": {},
   "source": [
    "- To avoid underflow when working with many small score values it makes sense to take their logarithm. \n",
    "- For convenience this can be done with the logscore method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ce75103-ab76-4543-b0b4-77d98ee39b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6520766965796932"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.logscore(\"never\", \"language is\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2864c57e-0273-4eab-9515-444ea38af5b1",
   "metadata": {},
   "source": [
    "### Text Generation using N-gram Language Model "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5b77444-25c6-4496-a756-019fa6974f7c",
   "metadata": {},
   "source": [
    "One cool feature of n-gram models is that they can be used to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61c93e83-28d3-44e7-8d41-517adac2b90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'this', 'gives', 'rise', 'to', 'many', 'parsing', 'errors.', 'researchers', 'including', 'brent', '(', '1993', ')', ',', '243⫺262.', 'briscoe', ',', 'ted', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(20, random_seed=8))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "623f7b00-6bda-4239-8f2c-44a5897c3acb",
   "metadata": {},
   "source": [
    "We can do some cleaning to the generated tokens to make it human-like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec2a42f8-982e-4f92-aeac-1923ea182d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer \n",
    "detokenize = TreebankWordDetokenizer().detokenize \n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    \"\"\" \n",
    "    :param model: An ngram language model from `nltk.lm.model`. \n",
    "    :param num_words: Max no. of words to generate. \n",
    "    :param random_seed: Seed value for random. \n",
    "    \"\"\" \n",
    "    content = [] \n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token) \n",
    "    return detokenize(content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6df1c07d-94a9-4fe0-a690-80e39dbdbd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and carroll used hypothesis testing has been used, and a half.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(model, 20, random_seed=7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7bc09bd0-c4d8-40db-a4d1-e5e06bf7bf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'scf-verb', 'link', 'is', 'motivated', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(28, random_seed=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e2e29864-1a18-42bc-99a2-3bf5b0ec73d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.40 next 320 items six crd 5.30 next 640 items finally av0 6.71 next 1280 items plants nn2 6.05 next'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(model, 20, random_seed=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8153e0f1-cefd-45da-a482-cc275f1eb237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'identical, the null hypothesis, h0 is false, there would be rejected, whereas the null hypthesis regarding'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(model, 20, random_seed=30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "362b4d78-f2f8-47f2-8009-2ccda00e892a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'association for computa- tional linguistics and linguistic theory 1⫺2 (2005), briscoe and carroll 1997: 360⫺36)'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(model, 20, random_seed=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5789e46-509b-4f44-98b6-931e514384a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and carroll used hypothesis testing has been used, and a half.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(model, 20, random_seed=7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "32e454bf-b83e-4067-8787-814a644e4165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'scf-verb', 'link', 'is', 'motivated', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(28, random_seed=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b25b90-796e-4ec3-8066-c4dde6451efe",
   "metadata": {},
   "source": [
    "### Saving the model "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ec8453d-b3d0-42ce-a201-df0fc69649a4",
   "metadata": {},
   "source": [
    "The native Python's pickle may not save the lambda functions in the model, so we can use the dill library in place of pickle to save and load the language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d947702-522f-4c37-933b-3827ddb6b24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mutual information and lexicography.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dill as pickle \n",
    " \n",
    "with open('kilgariff_ngram_model.pkl', 'wb') as fout:\n",
    "    pickle.dump(model, fout) \n",
    "with open('kilgariff_ngram_model.pkl', 'rb') as fin:\n",
    "    model_loaded = pickle.load(fin) \n",
    "    \n",
    "generate_sent(model_loaded, 20, random_seed=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd045b2-6294-439a-9bf6-7aa283b42796",
   "metadata": {},
   "source": [
    "**Perplexity:** \n",
    "- Perplexity is a common measure used to evaluate how well a language model predicts a sample of text.\n",
    "- It tells you, on average, how \"confused\" the model is when it tries to predict each word in the sequence.\n",
    "- Low perplexity means the model predicts the text well (it's less \"perplexed\").\n",
    "- High perplexity means the model finds the text unpredictable.\n",
    "- The formula for perplexity is $PP(W) = P(w_1, w_2, ..., w_N)^{-1/N}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ffdc3fd7-075d-4eec-87b7-3b962946e7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 3.014247178477014\n"
     ]
    }
   ],
   "source": [
    "#from nltk.lm import MLE\n",
    "#from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "# Training data\n",
    "train_text = [[\"this\", \"is\", \"a\", \"test\"], [\"this\", \"is\", \"another\", \"test\"]]\n",
    "\n",
    "# Prepare for trigram model\n",
    "n = 2\n",
    "train_data, vocab = padded_everygram_pipeline(n, train_text)\n",
    "\n",
    "# Train model\n",
    "model = MLE(n)\n",
    "model.fit(train_data, vocab)\n",
    "\n",
    "# Evaluate perplexity on a test sentence\n",
    "test_text = [\"this\", \"is\", \"a\", \"test\"]\n",
    "from nltk.lm.preprocessing import pad_both_ends, everygrams\n",
    "\n",
    "test_data = list(everygrams(pad_both_ends(test_text, n), max_len=n))\n",
    "print(\"Perplexity:\", model.perplexity(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14784648-582e-404f-be14-1aa25ad0ca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7995116195995315\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Preplexity for trigram model\n",
    "\n",
    "train_text = [\n",
    "    [\"I\", \"am\", \"Sam\"],\n",
    "    [\"Sam\", \"I\", \"am\"],\n",
    "    [\"I\", \"do\", \"not\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"]\n",
    "]\n",
    "\n",
    "n = 3  \n",
    "train_data, vocab = padded_everygram_pipeline(n, train_text)\n",
    "model = MLE(n)\n",
    "model.fit(train_data, vocab)\n",
    "\n",
    "test_text = [\"I\", \"am\", \"Sam\"]\n",
    "test_ngrams = list(everygrams(pad_both_ends(test_text, n), max_len=n))\n",
    "perplexity_value = model.perplexity(test_ngrams)\n",
    "print(perplexity_value)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e57513f-e514-406a-83f4-df18fa7596a6",
   "metadata": {},
   "source": [
    "# Evaluate which model is better bigram or trigram and reason why ?\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7bf263d1-3927-4d29-8ea1-fbe9abf5c860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built-in Perplexity: 2.7995116195995315\n",
      "Custom Perplexity: 1.4309690811052556\n"
     ]
    }
   ],
   "source": [
    "# Implement perplexity as a function using Python code and compare the result with the existing built-in method. [2 marks]\n",
    "import math\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends, everygrams\n",
    "\n",
    "train_text = [\n",
    "    [\"I\", \"am\", \"Sam\"],\n",
    "    [\"Sam\", \"I\", \"am\"],\n",
    "    [\"I\", \"do\", \"not\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"]\n",
    "]\n",
    "\n",
    "n = 3 \n",
    "train_data, vocab = padded_everygram_pipeline(n, train_text)\n",
    "\n",
    "# Train trigram model\n",
    "model = MLE(n)\n",
    "model.fit(train_data, vocab)\n",
    "\n",
    "# Test sentence\n",
    "test_text = [\"I\", \"am\", \"Sam\"]\n",
    "\n",
    "\n",
    "test_data = list(everygrams(pad_both_ends(test_text, n), max_len=n))\n",
    "print(\"Built-in Perplexity:\", model.perplexity(test_data))\n",
    "\n",
    "\n",
    "def custom_perplexity(model, test_tokens, n=3):\n",
    "    \"\"\"Compute perplexity manually using log probabilities from the model.\"\"\"\n",
    "    seq = list(pad_both_ends(test_tokens, n))\n",
    "    ngrams_seq = list(ngrams(seq, n))\n",
    "    log_prob_sum, M = 0.0, 0\n",
    "\n",
    "    for ng in ngrams_seq:\n",
    "        context, word = ng[:-1], ng[-1]\n",
    "        prob = model.score(word, context)\n",
    "        if prob == 0: \n",
    "            return float(\"inf\")\n",
    "        log_prob_sum += math.log(prob)\n",
    "        M += 1\n",
    "\n",
    "    return math.exp(-log_prob_sum / M)\n",
    "\n",
    "\n",
    "print(\"Custom Perplexity:\", custom_perplexity(model, test_text, n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b4960-d2ff-49ab-9abc-f7474e560a3d",
   "metadata": {},
   "source": [
    "### Handling Unseen Words in N-Gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d3979-ee23-4aa8-bae0-e676d3d61b9b",
   "metadata": {},
   "source": [
    "- **Drawback of MLE:** It assigns zero probability to any word or sequence not seen in training, which makes the model unusable for unseen data.\n",
    "- **Solution:** Use Smoothing technique\n",
    "- **Why smoothing is needed:** Methods like Laplace, Lidstone, and Witten-Bell adjust probabilities so that unseen words still get a small non-zero probability, making the model more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef29f3-19b9-4dcf-b9ea-9d2a8991f04f",
   "metadata": {},
   "source": [
    "### Question for Evaluation (10 M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8ddee-b50a-4001-a891-a2deaef2a597",
   "metadata": {},
   "source": [
    "**Evaluative Question:**\n",
    "\n",
    "Write a Python program that:\n",
    "1) Takes a paragraph from url: https://www.gutenberg.org/files/11/11-0.txt and save it in text file \"input_text.txt\" as input. (1 M)\n",
    "2) Preprocesses the text (tokenization, lowercasing). (1 M)\n",
    "3) Trains n-gram language models (bigram, trigram, 4-gram) using NLTK. (2 M)\n",
    "4) Shows analysis including frequency counts and example probability/score.  (2 M)\n",
    "5) Computes perplexity of each model and decides which model is best. (2 M)\n",
    "\n",
    "Note:- \n",
    "Use a free, long text (≥500 words) from Project Gutenberg (https://www.gutenberg.org/) or a long Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35d4be50-e468-4449-a591-258a69ced252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2-gram Model ===\n",
      "Sample frequency of word 'alice': 24\n",
      "P('alice' | 'said') = 0.6666666666666666\n",
      "\n",
      "=== 3-gram Model ===\n",
      "Sample frequency of word 'alice': 24\n",
      "P('alice' | 'said') = 0.6666666666666666\n",
      "\n",
      "=== 4-gram Model ===\n",
      "Sample frequency of word 'alice': 24\n",
      "P('alice' | 'said') = 0.6666666666666666\n",
      "Perplexity of 2-gram model: 5.776170184263713\n",
      "Perplexity of 3-gram model: 1.3219918535426098\n",
      "Perplexity of 4-gram model: 1.0456736592347409\n"
     ]
    }
   ],
   "source": [
    "import requests, re, math\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends, everygrams\n",
    "\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "text = requests.get(url).text\n",
    "open(\"input_text.txt\", \"w\", encoding=\"utf-8\").write(text)\n",
    "\n",
    "\n",
    "clean_text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "tokens = clean_text.split()[:2000]  \n",
    "\n",
    "\n",
    "models = {}\n",
    "for n in [2, 3, 4]:\n",
    "    train_data, vocab = padded_everygram_pipeline(n, [tokens])\n",
    "    model = MLE(n)\n",
    "    model.fit(train_data, vocab)\n",
    "    models[n] = model\n",
    "\n",
    "\n",
    "for n, model in models.items():\n",
    "    print(f\"\\n=== {n}-gram Model ===\")\n",
    "    print(\"Sample frequency of word 'alice':\", model.counts['alice'])\n",
    "    print(\"P('alice' | 'said') =\", model.score('alice', ['said']))\n",
    "\n",
    "\n",
    "def perplexity(model, tokens, n):\n",
    "    seq = list(pad_both_ends(tokens, n))\n",
    "    ngrams_seq = list(ngrams(seq, n))\n",
    "    log_prob, M = 0.0, 0\n",
    "    for ng in ngrams_seq:\n",
    "        context, word = ng[:-1], ng[-1]\n",
    "        prob = model.score(word, context)\n",
    "        if prob == 0:\n",
    "            return float(\"inf\")\n",
    "        log_prob += math.log(prob)\n",
    "        M += 1\n",
    "    return math.exp(-log_prob / M)\n",
    "\n",
    "for n, model in models.items():\n",
    "    pp = perplexity(model, tokens, n)\n",
    "    print(f\"Perplexity of {n}-gram model: {pp}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
